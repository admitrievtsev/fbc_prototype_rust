University of Minnesota, Twin-Cities Minneapolis, Minnesota, USA (lv, yjin, & du)@cs.umn.edu Abstract A predominant portion of Internet services, like
content delivery networks, news broadcasting, blogs sharing and
social networks, etc., is data centric. A significant amount of new
data is generated by these services each day. To efficiently store
and maintain backups for such data is a challenging task for
current data storage systems. Chunking based deduplication
(dedup) methods are widely used to eliminate redundant data
and hence reduce the required total storage space. In this paper,
we propose a novel Frequency Based Chunking (FBC) algorithm.
Unlike the most popular Content-Defined Chunking (CDC)
algorithm which divides the data stream randomly according to
the content, FBC explicitly utilizes the chunk frequency
information in the data stream to enhance the data deduplication
gain especially when the metadata overhead is taken into
consideration. The FBC algorithm consists of two components, a
statistical chunk frequency estimation algorithm for identifying
the globally appeared frequent chunks, and a two-stage chunking
algorithm which uses these chunk frequencies to obtain a better
chunking result. To evaluate the effectiveness of the proposed
FBC algorithm, we conducted extensive experiments on
heterogeneous datasets. In all experiments, the FBC algorithm
persistently outperforms the CDC algorithm in terms of
achieving a better dedup gain or producing much less number of
chunks. Particularly, our experiments show that FBC produces
2.5 ~ 4 times less number of chunks than that of a baseline CDC
which achieving the same Duplicate Elimination Ratio (DER).
Another benefit of FBC over CDC is that the FBC with average
chunk size greater than or equal to that of CDC achieves up to
50% higher DER than that of a CDC algorithm.
Keywords- Data Deduplication; Frequency based Chunking;
Content-defined Chunking; Statistical Chunk Frequency
Estimation; Bloom Filter
I. INTRODUCTION
Today, a predominant portion of Internet services, like
content delivery networks, news broadcasting, blogs sharing
and social networks, etc., is data centric. A significant amount
of new data is generated by these services each day. To
efficiently store and maintain backups for such data is a
challenging problem for current data storage systems.
In comparison to the compression technique which does not
support fast retrieval and modification of a specific data
segment, chunking based data deduplication (dedup) is
becoming a prevailing technology to reduce the space
requirement for both primary file systems and data backups. In
addition, it is well known that for certain backup datasets,
dedup technique could achieve a much higher dataset size
reduction ratio comparing to compression techniques such as
gzip (E.g. the size folding ratio of chunking based dedup to that
of gzip-only is 15:2. [25]). The basic idea for chunking based
data dedup methods is to divide the target data stream into a
number of chunks, the length of which could be either fixed
(Fix-Size Chunking, FSC) or variable (Content-Defined
Chunking, CDC). Among these resulted chunks, only one copy
of each unique chunk is stored and hence the required total
storage space is reduced.
The popular baseline CDC algorithm employs a
deterministic distinct sampling technique to determine the
chunk boundaries based on the data content. Specifically, CDC
algorithm adopts a fix-length sliding window to move onwards
the data stream byte by byte. If a data segment covered by the
sliding window sampled by the CDC algorithm satisfies certain
condition, it is marked as a chunk cut-point. The data segment
between the current cut-point and its preceding cut-point forms
a resultant chunk.
In this paper as a first step to assess the duplicate
elimination performance for chunking based dedup algorithms,
we advance the notion of data dedup gain. This metric takes
into account the metadata cost for a chunking algorithm and
hence enable us to compare different chunking algorithms in a
more realistic manner. Based on the data dedup gain metric, we
analyze the performance of the CDC algorithm in this paper.
Though being scalable and efficient, content defined chunking
is essentially a random chunking algorithm which does not
guarantee the appeared frequency of the resultant chunks and
hence may not be optimal for data dedup purpose. For
example, in order to reduce the storage space (i.e., to increase
the dedup gain), the only option for the CDC algorithm is to
reduce the average chunk size. That is, to increase the distinct
sampling rate to select more cut-points. However, when the
average chunk size is below a certain value, the gain in the
reduction of redundant chunks is diminished by the increase of
the metadata cost.
Being aware of the problems in the CDC algorithm, in this
paper, we propose a novel Frequency Based Chunking (FBC)
algorithm. The FBC algorithm is a two-stage algorithm. At the
first stage, the FBC applies the CDC algorithm to obtain
coarse-grained chunks, i.e., the resulted chunks are of a larger
average chunk size than desired. At the second stage, FBC
identifies the globally appeared frequent data segments from
each CDC chunk and used these data segments to further
selectively divide the CDC chunks into fine-grained chunks. In
this way, the FBC chunks consist of two parts, the globally
appeared frequent chunks and the chunks formed by the
remaining portions of the CDC chunks. Of course, an entire
CDC chunk may not be further partitioned if it does not contain
any frequently appeared data segments. Intuitively, the globally
2010 18th Annual IEEE/ACM International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems
2010 18th Annual IEEE/ACM International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems
Authorized licensed use limited to: St. Petersburg State University. Downloaded on April 06,2024 at 13:39:14 UTC from IEEE Xplore. Restrictions apply.
appeared frequent chunks are highly redundant and the
redundancy of the remaining chunks is close to that of the
original CDC chunks. Therefore, the FBC algorithm is
expected to have a better data dedup performance than the
CDC algorithm.
To obtain the globally appeared frequent chunks used by
the FBC algorithm, direct counting of the chunk frequencies is
not feasible due to the typically huge volume of the possible
data streams. In this paper, we employ a statistical chunk
frequency estimation algorithm for this task. Based on the
observation of the chunk frequency distribution in the data
stream, That is, high frequency chunks only account for a small
portion of all the data chunks. The proposed algorithm utilizes
a two-step filtering (a pre-filtering and a parallel filtering) to
eliminate the low frequency chunks and hence to reserve
resource to accurately identify the high frequent chunks. Due to
the powerful filtering mechanism, the proposed statistical
chunk frequency estimation algorithm only requires a few
megabytes to identify all the high frequency chunks with a
good precision, even though the threshold for defining the high
frequency chunks can be moderate (e.g., 15). Therefore, the
performance of the proposed algorithm is beyond the capability
of most state-of-the-art heavy hitter detection algorithms.
In order to validate the proposed FBC chunking algorithm,
we utilize data streams from heterogeneous sources.
Experimental results show that the FBC algorithm consistently
outperforms the CDC algorithm in terms of the dedup gain or
in terms of the number of chunks produced. Particularly, our
experiments verify that FBC produces 2.5 ~ 4 times less
number of chunks than that of a baseline CDC does when
achieving the same DER. FBC also outperforms CDC in terms
of DER. With the average chunk size no less than that of CDC,
FBC achieves up to 50% higher DER.
Our contribution in this paper can be summarized as
follows:
1) We advance the notion of data dedup gain to compare
different chunking algorithms in a realistic scenario. The metric
takes both the gain in data redundancy reduction and the
metadata overhead into consideration.
2) We propose a novel frequency based chunking
algorithm, which explicitly considers the frequency
information of data segments during the chunking process. To
the best of our knowledge, our work is the first one to
incorporate the frequency knowledge to the chunking process.
3) We design a statistical chunk frequency estimation
algorithm. With small memory footprint, the algorithm is able
to identify data chunks above a moderate frequency in a stream
setting. This algorithm itself can be applied to other application
domains as a streaming algorithm for cardinality estimation
purpose.
4) We conduct extensive experiments to evaluate the
proposed FBC algorithm using heterogeneous datasets, and the
FBC algorithm persistently outperforms the widely used CDC
algorithm.
The rest of the paper is organized as follows. In the next
section, we present an overview of the related work. Section III
provides the motivation of proposing a FBC algorithm. The
proposed FBC algorithm is described in Section IV with an
extensive analysis presented for each component. Section V
presents experimental results on several different types of
empirical datasets. Finally, we describe our future work and
some conclusions in Section VI.
II. RELATED WORK
As in the domain of data deduplication, chunking
algorithms can be mainly categorized into two classes: Fix-Size
Chunking (FSC) and Content-Defined Chunking (CDC). The
simplest and fastest approach is FSC which breaks the input
stream into fix-size chunks. FSC is used by rsync [1]. A major
problem with FSC is that editing (e.g. insert/delete) even a
single byte in a file will shift all chunk boundaries beyond that
edit point and result in a new version of the file having very
few repeated chunks. A storage system Venti [2] also adopts
FSC chunking for its simplicity. CDC derives chunks of
variable size and addresses boundary-shifting problem by
posing each chunk boundary only depending on the local data
content, not the offset in the stream. CDC will enclose the local
edits to a limited number of chunks. Both FSC and CDC help
data deduplication systems to achieve duplication elimination
by identifying repeated chunks. For any repeated chunks
identified, only one copy will be stored in the system.
CDC was first used to reduce the network traffic required
for remote file synchronization. Spring et al. [3] adopts
Borders [4] approach to devise the very first chunking
algorithm. It aims at identifying redundant network traffic.
Muthitacharoen et al. [5] presented a CDC based file system
called LBFS which extends the chunking approach to eliminate
data redundancy in low bandwidth networked file systems. You
et al. [6] adopts CDC algorithm to reduce the data redundancy
in an archival storage system. Some improvements to reduce
the variation of chunk sizes for CDC algorithm are discussed in
TTTD [7]. Recent research TAPER [8] and REBL [9]
demonstrate how to combine CDC and delta encoding [10] for
directory synchronization. Both schemes adopt a multi-tier
protocol which uses CDC and delta encoding for multi-level
redundancy detection. In fact, resemblance detection [4, 11]
combined with delta encoding [12] is usually a more aggressive
compression approach. However, finding similar or near
identical files [13] is not an easy task. Comparison results on
delta encoding and chunking are provided in [14, 15].
The concept of identifying high-frequency identities among
large population is discussed in the area of Internet traffic
analysis. Manku et al. [16] proposed an inferential algorithm to
find and count frequent item in the stream. However, their
work is very general and do not consider particular frequency
distribution of counted items. The idea of using parallel stream
filter to identify high cardinality Internet hosts among existing
host pairs is recently proposed in [17]. In this paper, we adopt
the concept of parallel stream filter and design a special parallel
bloom filter [18] based filter to identify high-frequency chunks
and to obtain their frequency estimates. We are also aware of
that bloom Filter was used in [19] and [20] to memorize data
chunks that have been observed. However, they use only one
bloom Filter and thus can only tell whether a chunk is observed
or not. Metadata overhead is a big concern as in chunking
288288
Authorized licensed use limited to: St. Petersburg State University. Downloaded on April 06,2024 at 13:39:14 UTC from IEEE Xplore. Restrictions apply.
based data deduplication [21]. Kruus et al. [20] presents a work
similar to ours. Provided chunk existence knowledge, they
propose a two-stage chunking algorithm that re-chunks
transitional and non-duplicated big CDC chunks into small
CDC chunks. The main contribution of their work is to be able
to reduce the number of chunks significantly while attaining as
the same duplicate elimination ratio as a baseline CDC
algorithm. Alternative work to reduce metadata overhead is to
perform local duplicate elimination within a relatively large
cluster of data. Extreme binning [22] detects file level content
similarity by a representative hash value, and shows to be able
achieving close to original CDC performance on data sets with
no locality among consecutive files. Another recent work [23]
presents a sparse indexing technique to detect similar large
segments within a stream.
III. M OTIVATION
In this section, we define the data dedup gain as a general
metric to measure the effectiveness of a dedup algorithm.
Based on this metric, we point out the problems in the popular
CDC algorithm which hinders the algorithm from achieving a
better dedup gain. This analysis motivates the proposed FBC
algorithm in Section IV.
Figure 3.1. Data Structures Related to Chunking Dedup
A. Data Dedup Gain

support file retrieval from stored chunks. Practically, SHA-1
hash algorithm is used to produce an ID for a chunk based on
its content. We store the de-duplicated chunks in a chunk store
and use an index table to map chunk IDs to the stored chunks.
Apparently, the benefit from dedup happens when we only
store one copy called unique chuck of the duplicated chunks
(e.g., chunk 1 in Fig. 3.1). However, we also need extra
storage space for the index table and chunk list, which we
refer to as the metadata overhead. Metadata is a system
dependent factor that affects the duplicate elimination result.
The final gain from the chunking-based dedup algorithm,
which we call the data dedup gain, is the difference between
the amount of duplicated chunks removed and the cost of the
metadata.
Denote the set of distinct chunks among these n chunks to
The first term in (3.1) is called the folding factor term which
stands for the gain by removing duplicated chunks. Since for
each unique chunk we only need to store one copy in the chunk
store and remove the rest of the copies, the folding factor is
calculated as the number of repeated copies of each unique
which is the sum of the length of on-disk chunk index plus the
We assume 20-byte SHA-1 hash is used to represent a
byte unit. It is worth noting that the index table size we
considered here is a theoretically lower-bound, so the
deduplication gain. (i.e., any real data deduplication system
definition assumes a chunking based data deduplication model
on top of the file system layer. Thus, the metadata overhead
referred to in this paper should not be confused with file system
metadata overhead such as inode, etc.
B. Discussion on the Baseline CDC algorithm
From the formula for the data dedup gain, given a fixed
number of chunks, a good chunking algorithm is expected to
generate more (and longer) frequent chunks to maximize the
dedup gain. Being aware of this, we study the baseline CDC
chunking algorithm.
After
chunking c1 c2 c3
ID1 ID2 ID3
chunk list
ID1 loc(c1)
ID2 loc(c2)
ID3 loc(c3)
Index table
s1 s2
de-duplicated
chunks (stored in
chunk store)
c1
c1 c3
ID1
c2
289289
Authorized licensed use limited to: St. Petersburg State University. Downloaded on April 06,2024 at 13:39:14 UTC from IEEE Xplore. Restrictions apply.